{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6057f4",
   "metadata": {},
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "\n",
    "dataDir = 'dataset'\n",
    "\n",
    "datasetUrl = 'https://archive.ics.uci.edu/static/public/45/heart+disease.zip'\n",
    "datasetZipFile = os.path.join(dataDir, 'heart-disease.zip')\n",
    "if not os.path.isdir(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "urllib.request.urlretrieve(datasetUrl, datasetZipFile)\n",
    "\n",
    "with ZipFile(datasetZipFile, 'r') as f:\n",
    "    f.extractall(dataDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d2552",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Need to determine whether it makes sense for columns with discrete values to be represented using ordinal encoding or one-hot encoding.\n",
    "If the values are based on binning a continuous variable, then the values are ordered as they should be.\n",
    "If the values are arbitrarily assigned based on category, then they should be transformed using something like one-hot encoding.\n",
    "\n",
    "Column descriptions taken from https://archive.ics.uci.edu/dataset/45/heart+disease:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ad2b8",
   "metadata": {},
   "source": [
    "age: age in years\n",
    "\n",
    "sex: sex (1 = male; 0 = female)\n",
    "\n",
    "cp: chest pain type\n",
    "\n",
    "        -- Value 1: typical angina\n",
    "\n",
    "        -- Value 2: atypical angina\n",
    "\n",
    "        -- Value 3: non-anginal pain\n",
    "\n",
    "        -- Value 4: asymptomatic\n",
    "\n",
    "trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "\n",
    "chol: serum cholestoral in mg/dl\n",
    "\n",
    "fbs: (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)\n",
    "\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "        -- Value 0: normal\n",
    "\n",
    "        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "\n",
    "thalach: maximum heart rate achieved\n",
    "\n",
    "exang: exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "oldpeak = ST depression induced by exercise relative to rest\n",
    "\n",
    "slope: the slope of the peak exercise ST segment\n",
    "\n",
    "        -- Value 1: upsloping\n",
    "\n",
    "        -- Value 2: flat\n",
    "\n",
    "        -- Value 3: downsloping\n",
    "\n",
    "ca: number of major vessels (0-3) colored by flourosopy\n",
    "\n",
    "thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "num: diagnosis of heart disease (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa5988",
   "metadata": {},
   "source": [
    "## Load the dataset and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918dcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandDataFile = os.path.join(dataDir, 'processed.cleveland.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnNames = [  # obtained from https://archive.ics.uci.edu/dataset/45/heart+disease\n",
    "    'age',\n",
    "    'sex',\n",
    "    'cp',\n",
    "    'trestbps',\n",
    "    'chol',\n",
    "    'fbs',\n",
    "    'restecg',\n",
    "    'thalach',\n",
    "    'exang',\n",
    "    'oldpeak',\n",
    "    'slope',\n",
    "    'ca',\n",
    "    'thal',\n",
    "    'num'\n",
    "]\n",
    "clevelandData = pd.read_csv(\n",
    "    clevelandDataFile,\n",
    "    header=None,\n",
    "    names=columnNames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f2816",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a3514",
   "metadata": {},
   "source": [
    "Columns 11 and 12 (`'ca'` and `'thal'`) are of Dtype `object`, which likely means that there are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adf98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb04b5f",
   "metadata": {},
   "source": [
    "Looking at histograms of the column values, one can make the following observations:\n",
    "- sex, cp, fbs, restecg, exang, slope, and num are discrete variables\n",
    "- trestbps, chol, and oldpeak might have outliers\n",
    "- trestbps appears to have much larger counts for a small set of discrete values, possibly indicating binning for some subset of the data\n",
    "- age could be multi-model\n",
    "- oldpeak has a much larger number of 0 values comapared to the others\n",
    "- restecg has a very small number of entries with value 1 compared to 0 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5820f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData.hist(bins=30, figsize=(15,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d8150",
   "metadata": {},
   "source": [
    "Looking at the counts for the unique values in columns `'ca'` and `'thal'`, we confirm that these columns appear to have missing values indicated by `'?'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ca0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData['ca'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData['thal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d6cf7",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e23a2d",
   "metadata": {},
   "source": [
    "#### Replace `'?'` with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData['ca'] = clevelandData['ca'].apply(lambda x: np.nan if x=='?' else np.float64(x))\n",
    "clevelandData['thal'] = clevelandData['thal'].apply(lambda x: np.nan if x=='?' else np.float64(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd09bb",
   "metadata": {},
   "source": [
    "#### Impute NaNs with the median\n",
    "Another option would be to drop the rows with NaNs since there are a small number of them).\n",
    "Question: does using the median make sense here? Would using the mode also make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86278d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(clevelandData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c659bc1",
   "metadata": {},
   "source": [
    "## Could also transform some columns to one-hot encoding if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03535865",
   "metadata": {},
   "source": [
    "Possible candidates for transform:\n",
    "\n",
    "sex\n",
    "\n",
    "cp: chest pain type\n",
    "\n",
    "        -- Value 1: typical angina\n",
    "\n",
    "        -- Value 2: atypical angina\n",
    "\n",
    "        -- Value 3: non-anginal pain\n",
    "\n",
    "        -- Value 4: asymptomatic\n",
    "\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "        -- Value 0: normal\n",
    "\n",
    "        -- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "        -- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "\n",
    "slope: the slope of the peak exercise ST segment\n",
    "\n",
    "        -- Value 1: upsloping\n",
    "\n",
    "        -- Value 2: flat\n",
    "\n",
    "        -- Value 3: downsloping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef13ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\n",
    "    'age',\n",
    "    'trestbps',\n",
    "    'chol',\n",
    "    'fbs',\n",
    "    'thalach',\n",
    "    'exang',\n",
    "    'oldpeak',\n",
    "    'ca',\n",
    "    'thal'\n",
    "]\n",
    "categorical_cols = ['sex', 'cp', 'restecg', 'slope']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d2042",
   "metadata": {},
   "source": [
    "## Make target variable binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e71f3a",
   "metadata": {},
   "source": [
    "Based on the dataset description, any entries with values of the `num` column that are greater than zero should be considered as having heart disease, and entries with values of zero should be considered as not having heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData['target'] = clevelandData['num'].apply(lambda x: 1 if x > 0 else 0)\n",
    "clevelandData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e833330",
   "metadata": {},
   "source": [
    "## Create a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1234)\n",
    "for idx_train, idx_test in splitter.split(clevelandData, clevelandData['num']):\n",
    "    clevelandData_train = clevelandData.loc[idx_train]\n",
    "    clevelandData_test = clevelandData.loc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData_train['num'].value_counts() / clevelandData_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bff91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData_test['num'].value_counts() / clevelandData_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20deea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "clevelandData['num'].value_counts() / clevelandData.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca0dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (clevelandData_train, clevelandData_test):\n",
    "    set_.drop(\"num\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360a057",
   "metadata": {},
   "source": [
    "## Look for correlations between columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21963a30",
   "metadata": {},
   "source": [
    "Interestingly, the target variable has very low correlation with respect to cholesterol level!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540eb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = clevelandData_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix['target'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd027989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\n",
    "    'target', 'thal', 'exang', 'ca', 'oldpeak', 'cp', 'slope', 'age', 'sex',\n",
    "    'trestbps', 'restecg', 'thalach'\n",
    "]\n",
    "scatter_matrix(\n",
    "    clevelandData_train[attributes],\n",
    "    figsize=(12,12)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce23ea7",
   "metadata": {},
   "source": [
    "## Build pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = clevelandData_train['target'].copy().to_numpy()\n",
    "clevelandData_train_X = clevelandData_train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286250e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, numerical_cols),\n",
    "    ('cat', OneHotEncoder(), categorical_cols)\n",
    "])\n",
    "\n",
    "train_X = full_pipeline.fit_transform(clevelandData_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a503bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef301953",
   "metadata": {},
   "source": [
    "## Train and evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd077089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cdca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=1234)\n",
    "scores = cross_val_score(tree_clf, train_X, train_y,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=1234)\n",
    "forest_scores = cross_val_score(forest_clf, train_X, train_y,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c91756",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear', random_state=1234)\n",
    "svm_scores = cross_val_score(svm_clf, train_X, train_y,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "svm_rmse_scores = np.sqrt(-svm_scores)\n",
    "display_scores(svm_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438b5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', random_state=1234)  # L2 regularization is used by default\n",
    "log_reg_scores = cross_val_score(log_reg, train_X, train_y,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=10)\n",
    "log_reg_rmse_scores = np.sqrt(-log_reg_scores)\n",
    "display_scores(log_reg_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826ea53b",
   "metadata": {},
   "source": [
    "## Grid search hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c75081",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    # try 7 combinations of hyperparameters\n",
    "    {'C': [1/10, 1/4, 1/2, 1, 2, 4, 10],\n",
    "     'penalty': ['l2', 'l1']},\n",
    "  ]\n",
    "\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=1234)\n",
    "grid_search = GridSearchCV(log_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2598d4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "cvres = grid_search.cv_results_\n",
    "idx = np.argsort(cvres['mean_test_score'])[::-1]\n",
    "for k in idx:\n",
    "    print(\n",
    "        'Test score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_test_score'][k]), np.sqrt(cvres['std_test_score'][k])),\n",
    "        'Train score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_train_score'][k]), np.sqrt(cvres['std_train_score'][k])),\n",
    "        cvres['params'][k]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b800a2",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a18ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C': [1/10, 1/4, 1/2, 1, 2, 4, 10],\n",
    "     'kernel': ['linear', 'rbf']},\n",
    "  ]\n",
    "\n",
    "svm_clf = SVC(random_state=1234)\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "cvres = grid_search.cv_results_\n",
    "idx = np.argsort(cvres['mean_test_score'])[::-1]\n",
    "for k in idx:\n",
    "    print(\n",
    "        'Test score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_test_score'][k]), np.sqrt(cvres['std_test_score'][k])),\n",
    "        'Train score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_train_score'][k]), np.sqrt(cvres['std_train_score'][k])),\n",
    "        cvres['params'][k]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b673f88",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edfbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30, 100],\n",
    "     'max_features': [2, 4, 6, 8, 12, 14],\n",
    "     'max_depth': [None, 2, 4, 16, 32]},\n",
    "  ]\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=1234)\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "cvres = grid_search.cv_results_\n",
    "idx = np.argsort(cvres['mean_test_score'])[::-1]\n",
    "for k in idx:\n",
    "    print(\n",
    "        'Test score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_test_score'][k]), np.sqrt(cvres['std_test_score'][k])),\n",
    "        'Train score = %.3f+\\-%.3f' % (np.sqrt(-cvres['mean_train_score'][k]), np.sqrt(cvres['std_train_score'][k])),\n",
    "        cvres['params'][k]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "sorted(zip(feature_importances, clevelandData_train_X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b8cb21",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70799226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y, random_state):\n",
    "    splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "    for idx_train, idx_val in splitter.split(X, y):\n",
    "        X_train = X[idx_train]\n",
    "        X_val = X[idx_val]\n",
    "        y_train = y[idx_train]\n",
    "        y_val = y[idx_val]\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(10, len(X_train) + 1):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    ax.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=14)\n",
    "    ax.set_xlabel(\"Training set size\", fontsize=14)\n",
    "    ax.set_ylabel(\"RMSE\", fontsize=14)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38490857",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=1234)  # L2 regularization is used by default\n",
    "fig, ax = plot_learning_curves(log_reg, train_X, train_y, random_state=1234)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ba418",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear', C=0.5, random_state=1234)\n",
    "fig, ax = plot_learning_curves(svm_clf, train_X, train_y, random_state=1234)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67429927",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(max_depth=2, max_features=6, n_estimators=10, random_state=1234)\n",
    "fig, ax = plot_learning_curves(forest_clf, train_X, train_y, random_state=1234)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e9c16",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e1d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=1234)\n",
    "y_train_pred = cross_val_predict(log_reg, train_X, train_y, cv=10)\n",
    "conf_mat = confusion_matrix(train_y, y_train_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "TP, FP, FN, TN = conf_mat[0,0], conf_mat[0,1], conf_mat[1,0], conf_mat[1,1]\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('acc = %.3f' % acc)\n",
    "print('precision = %.3f' % precision)\n",
    "print('recall = %.3f' % recall)\n",
    "print('f1 = %.3f' % f1_score(train_y, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1284ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear', C=0.5, random_state=1234)\n",
    "y_train_pred = cross_val_predict(svm_clf, train_X, train_y, cv=10)\n",
    "conf_mat = confusion_matrix(train_y, y_train_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "TP, FP, FN, TN = conf_mat[0,0], conf_mat[0,1], conf_mat[1,0], conf_mat[1,1]\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('acc = %.3f' % acc)\n",
    "print('precision = %.3f' % precision)\n",
    "print('recall = %.3f' % recall)\n",
    "print('f1 = %.3f' % f1_score(train_y, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(max_depth=2, max_features=6, n_estimators=10, random_state=1234)\n",
    "y_train_pred = cross_val_predict(forest_clf, train_X, train_y, cv=10)\n",
    "conf_mat = confusion_matrix(train_y, y_train_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "TP, FP, FN, TN = conf_mat[0,0], conf_mat[0,1], conf_mat[1,0], conf_mat[1,1]\n",
    "TP, FP, FN, TN = conf_mat[0,0], conf_mat[0,1], conf_mat[1,0], conf_mat[1,1]\n",
    "recall = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "print('acc = %.3f' % acc)\n",
    "print('precision = %.3f' % precision)\n",
    "print('recall = %.3f' % recall)\n",
    "print('f1 = %.3f' % f1_score(train_y, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86202d2b",
   "metadata": {},
   "source": [
    "## Precision vs. recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_precision_vs_recall(precisions, recalls, ax, colorAndProps):\n",
    "    ax.plot(recalls, precisions, colorAndProps, linewidth=2)\n",
    "    ax.set_xlabel(\"Recall\", fontsize=16)\n",
    "    ax.set_ylabel(\"Precision\", fontsize=16)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# logistic regression\n",
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=1234)\n",
    "y_train_scores = cross_val_predict(log_reg, train_X, train_y, cv=10,\n",
    "                             method=\"decision_function\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(train_y, y_train_scores)\n",
    "plot_precision_vs_recall(precisions, recalls, ax, 'r-')\n",
    "\n",
    "# SVM\n",
    "svm_clf = SVC(kernel='linear', C=0.5, random_state=1234)\n",
    "y_train_scores = cross_val_predict(svm_clf, train_X, train_y, cv=10,\n",
    "                             method=\"decision_function\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(train_y, y_train_scores)\n",
    "plot_precision_vs_recall(precisions, recalls, ax, 'b-')\n",
    "\n",
    "# random forest\n",
    "forest_clf = RandomForestClassifier(max_depth=2, max_features=6, n_estimators=10, random_state=1234)\n",
    "y_train_scores = cross_val_predict(forest_clf, train_X, train_y, cv=10,\n",
    "                             method=\"predict_proba\")\n",
    "precisions, recalls, thresholds = precision_recall_curve(train_y, y_train_scores[:,1])\n",
    "plot_precision_vs_recall(precisions, recalls, ax, 'k-')\n",
    "\n",
    "ax.legend(['Log reg', 'SVM', 'RF'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c92f18",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbacae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "train_X_pca = pca.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.cumsum(pca.explained_variance_ratio_), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dbfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=1234)  # L2 regularization is used by default\n",
    "log_reg_scores = cross_val_score(log_reg, train_X_pca[:,:10], train_y,\n",
    "                                scoring=\"neg_mean_squared_error\", cv=5)\n",
    "log_reg_rmse_scores = np.sqrt(-log_reg_scores)\n",
    "display_scores(log_reg_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "train_X_tsne = TSNE(\n",
    "    n_components=2,\n",
    "    learning_rate='auto',\n",
    "    init='pca',\n",
    "    perplexity=50\n",
    ").fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    train_X_tsne[train_y==0,0],\n",
    "    train_X_tsne[train_y==0,1],\n",
    "    'bo'\n",
    ")\n",
    "ax.plot(\n",
    "    train_X_tsne[train_y==1,0],\n",
    "    train_X_tsne[train_y==1,1],\n",
    "    'ro'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a762c",
   "metadata": {},
   "source": [
    "## Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4af1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = clevelandData_test['target'].copy().to_numpy()\n",
    "clevelandData_test_X = clevelandData_test.drop('target', axis=1)\n",
    "test_X = full_pipeline.fit_transform(clevelandData_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415e7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=1234)\n",
    "forest_clf.fit(train_X, train_y)\n",
    "print(f'acc = {forest_clf.score(test_X, test_y)}')\n",
    "print(f'f1 = {f1_score(test_y, forest_clf.predict(test_X))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=1234)\n",
    "log_reg.fit(train_X, train_y)\n",
    "print(f'acc = {log_reg.score(test_X, test_y)}')\n",
    "print(f'f1 = {f1_score(test_y, log_reg.predict(test_X))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35226f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(kernel='linear', C=0.5, random_state=1234)\n",
    "svm_clf.fit(train_X, train_y)\n",
    "print(f'acc = {svm_clf.score(test_X, test_y)}')\n",
    "print(f'f1 = {f1_score(test_y, svm_clf.predict(test_X))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7b843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
